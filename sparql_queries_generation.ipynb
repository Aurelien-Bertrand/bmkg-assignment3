{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BMKG - Assignment 3\n",
    "\n",
    "This notebook intends to automatically generate a SCHEMA for any given KG."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1ecf4af2ac11dae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from rdflib import Graph\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate, format_document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_openai import OpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T13:21:03.499800Z",
     "start_time": "2024-03-24T13:21:03.098494Z"
    }
   },
   "id": "616379e27251c82f",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API Key\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T13:10:02.824588Z",
     "start_time": "2024-03-24T13:09:45.753560Z"
    }
   },
   "id": "de00d5a25be72804",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) Get the RDF graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b25c0fea03e406"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples:  178548\n"
     ]
    }
   ],
   "source": [
    "schema = Graph()\n",
    "schema.parse(\"../Assignment 2/climate_kg_ontology.ttl\")\n",
    "\n",
    "graph = Graph()\n",
    "graph = graph.parse(\"../Assignment 1/src/graph_from_data_sources.ttl\")\n",
    "\n",
    "print(\"Number of triples: \", len(graph))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T14:18:45.094267Z",
     "start_time": "2024-03-24T14:18:39.793769Z"
    }
   },
   "id": "d56cc8e9240d6e3b",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T14:26:25.406785Z",
     "start_time": "2024-03-24T14:26:25.387824Z"
    }
   },
   "id": "7be82a9bb3943d7f",
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create the memory object that is used to add messages\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "# Add a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "# Prompt to reformulate the question using the chat history\n",
    "reform_template = \"\"\"Given the following chat history and a follow up question,\n",
    "rephrase the follow up question to be a standalone straightforward question, in its original language.\n",
    "Do not answer the question! Just rephrase reusing informations from the chat history.\n",
    "Make it short and straight to the point.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow up input:\n",
    "{question}\n",
    "\n",
    "Standalone question:\n",
    "\"\"\"\n",
    "REFORM_QUESTION_PROMPT = PromptTemplate.from_template(reform_template)\n",
    "\n",
    "# Prompt to ask to answer the reformulated question\n",
    "answer_template = \"\"\"Given the following chat history and the schema \n",
    "of an RDF graph, generate a SPARQL (including prefixes) query to \n",
    "answer the question. Make sure to include only what is part of the schema,\n",
    "so it is fine if you do not answer entirely the question, only the part that\n",
    "is actually part of the schema should be included:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T14:26:25.682457Z",
     "start_time": "2024-03-24T14:26:25.679935Z"
    }
   },
   "id": "bf55da9ace49a765",
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reformulate the question using chat history\n",
    "reformulated_question = {\n",
    "    \"reformulated_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "        \"schema\": lambda x: x[\"schema\"],\n",
    "    }\n",
    "    | REFORM_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Construct the inputs for the final prompt using retrieved documents\n",
    "final_inputs = {\n",
    "    \"schema\": lambda x: x[\"reformulated_question\"],\n",
    "    \"question\": lambda x: print(\"ðŸ’­ Reformulated question:\", x[\"reformulated_question\"]) or x[\"reformulated_question\"],\n",
    "}\n",
    "# Generate the answer using the retrieved documents and answer prompt\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "}\n",
    "# Put the chain together\n",
    "final_chain = loaded_memory | reformulated_question | answer\n",
    "\n",
    "def stream_chain(final_chain, memory: ConversationBufferMemory, inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    \"\"\"Ask question, stream the answer output, and return the answer with source documents.\"\"\"\n",
    "    output = {\"answer\": \"\"}\n",
    "    for chunk in final_chain.stream(inputs):\n",
    "        if \"answer\" in chunk:\n",
    "            output[\"answer\"] += chunk[\"answer\"]\n",
    "            print(chunk[\"answer\"], end=\"\", flush=True)\n",
    "    # Add messages to chat history\n",
    "    memory.save_context(inputs, {\"answer\": output[\"answer\"]})\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T14:26:26.647749Z",
     "start_time": "2024-03-24T14:26:26.644677Z"
    }
   },
   "id": "28518303c93282ec",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ Reformulated question: What is the label of Belgium?\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "SELECT ?label\n",
      "WHERE {\n",
      "  ?country rdf:type <http://example.org/Country> .\n",
      "  ?country rdfs:label ?label .\n",
      "  FILTER (?country = <http://example.org/Belgium>)\n",
      "}"
     ]
    }
   ],
   "source": [
    "output = stream_chain(final_chain, memory, {\"schema\": str(schema.serialize(format=\"ttl\")), \"question\": \"What is the label of the country Belgium?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T14:26:30.083329Z",
     "start_time": "2024-03-24T14:26:27.243073Z"
    }
   },
   "id": "5ee43039f6f4af97",
   "execution_count": 113
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
